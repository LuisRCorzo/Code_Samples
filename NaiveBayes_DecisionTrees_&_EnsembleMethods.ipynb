{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes, Decision Trees with ensemble methods\n",
    "## CSCI 4622 - Fall 2021\n",
    "***\n",
    "**Name**: $Luis R. Corzo$ \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "Your task for this homework is to build a naive Bayes and a decision tree classifiers in the first 2 problems.\n",
    "The last problem is about ensemble methods using scikit-learn decision tree as a weak learner.\n",
    "We'll explore bagging and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Naive Bayes [25 points]\n",
    "***\n",
    "Consider the problem of predicting whether a person has a college degree based on age, salary, and Colorado residency.\n",
    "The dataset looks like the following.\n",
    "\n",
    "|Age|Salary|Colorado Residency| College degree|\n",
    "|:------:|:-----------:| :----------:|--:|\n",
    "| 27 | 41,000 | Yes | Yes |\n",
    "| 61 | 52,000 | No | No |\n",
    "| 23 | 24,000 | Yes | No |\n",
    "| 29 | 77,000 | Yes | Yes |\n",
    "| 32 | 48,000 | No | Yes |\n",
    "| 57 | 120,000 | Yes | Yes |\n",
    "| 22 | 38,000 | Yes | Yes |\n",
    "| 41 | 45,000 | Yes | No |\n",
    "| 53 | 26,000 | No | No |\n",
    "| 48 | 65,000 | Yes | Yes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = np.array([[27 , 41000 , 1],\n",
    "              [61 , 52000 , 0],\n",
    "              [23 , 24000 , 1],\n",
    "              [29 , 77000 , 1],\n",
    "              [32 , 48000 , 0],\n",
    "              [57 , 120000 , 1],\n",
    "              [22 , 38000 , 1],\n",
    "              [41 , 45000 , 1],\n",
    "              [53 , 26000 , 0],\n",
    "              [48 , 65000 , 1]])\n",
    "\n",
    "labels = np.array([1, 0, 0, 1, 1, 1, 1, 0, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1 What is our expected accuracy for the baseline case where we predict one label for all rows? (*2 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#BEGIN Workspace 1.1\n",
    "\n",
    "    I will be using the zero rule algorithm to predict the accuracy for the baseline case. Out of the ten samples in our data set, six have the label \"yes\" and four have the label \"no\". (-six samples have a college degree and four do not.) According to the ZeroR benchmark procedure, our expected baseline accuracy is 60%. If we predicted all rows to have \"yes\" as the label, our accuracy would be of 60%.\n",
    "\n",
    "#END Workspace 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we have to find a way to deal with the continuous features. For now, let's put them into binary bins based on threshold arguments to our classifier - so we can treat this as a tuning parameter.\n",
    "\n",
    "1.2 Complete `threshold_features` to convert age and salary features to binary ones using the threshold arguments. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def threshold_features(features, age_threshold, salary_threshold):\n",
    "    binary_X = features * 1 #This row just creates a \"hard copy\" of the X array so we can manipulate it as needed\n",
    "\n",
    "    #BEGIN Workspace 1.2\n",
    "    #TODO: Threshold the corresponding features\n",
    "\n",
    "    for sample in binary_X: \n",
    "        \n",
    "        if sample[0] < age_threshold:\n",
    "            sample[0] = 0\n",
    "        else:\n",
    "            sample[0] = 1\n",
    "            \n",
    "        if sample[1] < salary_threshold:\n",
    "            sample[1] = 0\n",
    "        else:\n",
    "            sample[1] = 1\n",
    "        \n",
    "    \n",
    "    #END Workspace 1.2\n",
    "\n",
    "    return binary_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As seen during the class, given a row $(x_1, x_2, x_3)$, the naive Bayes classifier should assign the label $y$ that\n",
    "maximizes:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\log [p(y) \\prod_i p(x_i | y)] = \\log p(y) + \\sum_{i} \\log p(x_i | y)\n",
    "\\end{align}$$\n",
    "\n",
    "$p(y)$ and $p(x_i | y)$ are computed using the training set (during `fit` call).\n",
    "\n",
    "We have defined $p(x_i | y)$ as :\n",
    "\\begin{align}\n",
    "p(x_i | y) = \\frac{N_{y,i}}{N_y}\n",
    "\\end{align}\n",
    "where $N_{y,i}$ is the number of rows where $y$ and $x_i$ occur together, and $N_y = \\sum_i N_{y,i}$.\n",
    "\n",
    "1.3 Complete the `fit` call by computing the counts and joint counts. Hint: Use `features_counts` to store the contingency\n",
    "table $N_{y,i}$ for each feature $i$ and then use them to compute $\\log p(x_i | y)$ (*10 points*)\n",
    "\n",
    "1.4 Complete the `predict` call (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    \"\"\"\n",
    "    NaiveBayes classifier for binary features and binary labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes_counts = None\n",
    "        self.features_counts = []\n",
    "        self.classes_log_probabilities = None\n",
    "        self.features_log_probabilities = [] # same structure as features_count\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: binary np.array of shape (n_samples, n_features)\n",
    "        y: corresponding labels of shape (n_samples,)\n",
    "        Returns\n",
    "        -------\n",
    "        Trained classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        #BEGIN Workspace 1.3\n",
    "        #TODO: Compute the counts and joint counts\n",
    "        \n",
    "        yes_counts = 0\n",
    "        no_counts = 0\n",
    "        \n",
    "        for label in y:\n",
    "            \n",
    "            if label == 1:\n",
    "                yes_counts += 1\n",
    "            if label == 0:\n",
    "                no_counts += 1\n",
    "        \n",
    "        counts = np.array([yes_counts, no_counts])\n",
    "        \n",
    "        self.classes_counts = counts\n",
    "        \n",
    "        log_p_y = np.array([np.log((yes_counts+1)/(len(y)+2)), np.log((no_counts+1)/(len(y)+2))])\n",
    "        \n",
    "        self.classes_log_probabilities = log_p_y\n",
    "    \n",
    "        log_p_x1 = 0\n",
    "        log_p_x2 = 0\n",
    "        log_p_x3 = 0\n",
    "        \n",
    "        # joint counts stored are as of the form N_y_xi = [N_yesi, N_noi], \n",
    "        # where N_yesi = [(# of counts where xi=1 and y=yes), (# of counts where xi=0 and y=yes)],\n",
    "        # and N_noi = [(# of counts where xi=1 and y=no), (# of counts where xi=0 and y=no)]\n",
    "        \n",
    "        N_yes1 = np.array([0,0]) \n",
    "        N_yes2 = np.array([0,0])\n",
    "        N_yes3 = np.array([0,0])\n",
    "        \n",
    "        N_no1 = np.array([0,0])\n",
    "        N_no2 = np.array([0,0])\n",
    "        N_no3 = np.array([0,0])\n",
    "        \n",
    "        for (row, label) in zip(X, y):\n",
    "            \n",
    "            #x1 feature of sample\n",
    "            if row[0] == 1:       #x1 feature is above threshold \n",
    "                \n",
    "                if label == 1: \n",
    "                    N_yes1[0] += 1\n",
    "            \n",
    "                if label == 0:\n",
    "                    N_no1[0] += 1\n",
    "            \n",
    "            if row[0] == 0:       #x1 feature is under threshold \n",
    "                \n",
    "                if label == 1:\n",
    "                    N_yes1[1] += 1\n",
    "            \n",
    "                if label == 0:\n",
    "                    N_no1[1] += 1\n",
    "            \n",
    "            \n",
    "            #x2 feature of sample\n",
    "            if row[1] == 1:\n",
    "                \n",
    "                if label == 1:\n",
    "                    N_yes2[0] += 1\n",
    "            \n",
    "                if label == 0:\n",
    "                    N_no2[0] += 1\n",
    "            \n",
    "            if row[1] == 0:\n",
    "                \n",
    "                if label == 1:\n",
    "                    N_yes2[1] += 1\n",
    "            \n",
    "                if label == 0:\n",
    "                    N_no2[1] += 1\n",
    "            \n",
    "            \n",
    "            #x3 feature of sample\n",
    "            if row[2] == 1:\n",
    "                \n",
    "                if label == 1:\n",
    "                    N_yes3[0] += 1\n",
    "            \n",
    "                if label == 0:\n",
    "                    N_no3[0] += 1\n",
    "                \n",
    "            if row[2] == 0:\n",
    "                \n",
    "                if label == 1:\n",
    "                    N_yes3[1] += 1\n",
    "            \n",
    "                if label == 0:\n",
    "                    N_no3[1] += 1\n",
    "                    \n",
    "        N_y_x1 = np.array([N_yes1, N_no1])\n",
    "        N_y_x2 = np.array([N_yes2, N_no2])\n",
    "        N_y_x3 = np.array([N_yes3, N_no3])\n",
    "        \n",
    "        N_yes = np.array([ [N_yes1[0] + N_yes2[0] + N_yes3[0]], [N_yes1[1] + N_yes2[1] + N_yes3[1]] ])\n",
    "        N_no = np.array([ [N_no1[0] + N_no2[0] + N_no3[0]], [N_no1[1] + N_no2[1] + N_no3[1]] ])\n",
    "        \n",
    "        N_y_xi = np.array([N_y_x1, N_y_x2, N_y_x3])\n",
    "        \n",
    "        self.features_counts = N_y_xi\n",
    "        \n",
    "        \n",
    "        log_p_x1_yes = np.array([np.log((N_yes1[0]+self.alpha)/(float(N_yes[0]) + 2*self.alpha)), np.log((N_yes1[1]+self.alpha)/(float(N_yes[1]) + 2*self.alpha))])\n",
    "        log_p_x2_yes = np.array([np.log((N_yes2[0]+self.alpha)/(float(N_yes[0]) + 2*self.alpha)), np.log((N_yes2[1]+self.alpha)/(float(N_yes[1]) + 2*self.alpha))])\n",
    "        log_p_x3_yes = np.array([np.log((N_yes3[0]+self.alpha)/(float(N_yes[0]) + 2*self.alpha)), np.log((N_yes3[1]+self.alpha)/(float(N_yes[1]) + 2*self.alpha))])\n",
    "        \n",
    "        log_p_x1_no = np.array([np.log((N_no1[0]+self.alpha)/(float(N_no[0]) + 2*self.alpha)), np.log((N_no1[1]+self.alpha)/(float(N_no[1]) + 2*self.alpha))])\n",
    "        log_p_x2_no = np.array([np.log((N_no2[0]+self.alpha)/(float(N_no[0]) + 2*self.alpha)), np.log((N_no2[1]+self.alpha)/(float(N_no[1]) + 2*self.alpha))])\n",
    "        log_p_x3_no = np.array([np.log((N_no3[0]+self.alpha)/(float(N_no[0]) + 2*self.alpha)), np.log((N_no3[1]+self.alpha)/(float(N_no[1]) + 2*self.alpha))])\n",
    "        \n",
    "        log_p_xi = np.array([log_p_x1_yes, log_p_x2_yes, log_p_x3_yes, log_p_x1_no, log_p_x2_no, log_p_x3_no])\n",
    "        \n",
    "        self.features_log_probabilities = log_p_xi\n",
    "        \n",
    "        #END Workspace 1.3\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        \n",
    "        joint_log_likelihood = np.zeros((x_test.shape[0], self.classes_counts.shape[0]))\n",
    "        y_hat = []\n",
    "        #BEGIN Workspace 1.4\n",
    "        #TODO: Find the corresponding labels using Naive bayes logic\n",
    "        \n",
    "        for sample in x_test:\n",
    "            \n",
    "            x1 = sample[0]\n",
    "            x2 = sample[1]\n",
    "            x3 = sample[2]\n",
    "\n",
    "            log_p_yes = self.classes_log_probabilities[0]\n",
    "\n",
    "            log_p_x1_yes = self.features_log_probabilities[0][1-x1]\n",
    "\n",
    "            log_p_x2_yes = self.features_log_probabilities[1][1-x2]\n",
    "\n",
    "            log_p_x3_yes = self.features_log_probabilities[2][1-x3]\n",
    "\n",
    "            p_yes_X = log_p_yes + log_p_x1_yes + log_p_x2_yes + log_p_x3_yes\n",
    "\n",
    "\n",
    "            log_p_no = self.classes_log_probabilities[1]\n",
    "\n",
    "            log_p_x1_no = self.features_log_probabilities[3][1-x1]\n",
    "\n",
    "            log_p_x2_no = self.features_log_probabilities[4][1-x2]\n",
    "\n",
    "            log_p_x3_no = self.features_log_probabilities[5][1-x3]\n",
    "\n",
    "            p_no_X = log_p_no + log_p_x1_no + log_p_x2_no + log_p_x3_no\n",
    "\n",
    "            if p_no_X < p_yes_X:\n",
    "                y_hat.append(1)\n",
    "\n",
    "            else:\n",
    "                y_hat.append(0)\n",
    "        \n",
    "        #END Workspace 1.4\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.5 Using age 30 and salary 40,000 as thresholds, transform the features and evaluate (accuracy) the NaiveBayes classifier\n",
    "on the training data. (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels:    [1, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "Predicted labels: [1, 0, 1, 1, 0, 1, 1, 1, 0, 1]\n",
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayes(alpha=1)\n",
    "#BEGIN Workspace 1.5\n",
    "#TODO: Transform features to binary features, fit the classifier, report the accuracy\n",
    "\n",
    "bin_features = threshold_features(features, 30, 40000)\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(bin_features, labels, test_size=0.2)\n",
    "\n",
    "print(\"Actual labels:   \", list(labels))\n",
    "\n",
    "clf.fit(bin_features, labels)\n",
    "pred_y = clf.predict(bin_features)\n",
    "\n",
    "print(\"Predicted labels:\", pred_y)\n",
    "hit, miss = 0, 0\n",
    "\n",
    "for (pred_label, label) in zip(pred_y, labels):\n",
    "    \n",
    "    if pred_label == label:\n",
    "        hit += 1\n",
    "    else:\n",
    "        miss += 1\n",
    "\n",
    "accuracy = hit/(hit+miss)    \n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#END Workspace 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Bonus question** 1.6 Use the attribute `alpha` of the NaiveBayes to convert it to the smoothed NaiveBayes presented during the class. (*5 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 2 - Decision trees [25 points]\n",
    "***\n",
    "The goal of this problem is to implement the core elements of the Decision Tree classifier.\n",
    "We do not expect a highly efficient implementation of the functions since the ensemble methods will use the implementation\n",
    "from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by considering the variable *Colorado residency*.\n",
    "\n",
    "The leaf nodes of a decision tree act in the same way as in question (1.1) where no feature is used.\n",
    "\n",
    "2.1 Complete `get_error_in_leaf` to return the count of misclassified instances. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_error_in_leaf(y, indices):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param indices: the subset of indexes in the leaf node\n",
    "    :return: Returns the number of errors in a leaf node of a decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    error_count = 0\n",
    "    #BEGIN Workspace 2.1\n",
    "    #TODO: Compute the number of errors in the leaf node (no feature is used)\n",
    "    \n",
    "    leaf = np.array([y[i] for i in indices])\n",
    "    \n",
    "    ones = np.count_nonzero(leaf)\n",
    "    zeros = len(leaf) - ones\n",
    "    \n",
    "    if ones >= zeros:\n",
    "        error_count = zeros\n",
    "    else:\n",
    "        error_count = ones\n",
    "    \n",
    "    #END Workspace 2.1\n",
    "    return error_count\n",
    "\n",
    "\n",
    "def value_split_binary_feature(x, y, feature_index, root, criteria_func):  #splits parent node (root) and returns criteria value (information gain)\n",
    "    \"\"\"Will be used later to evaluate the criteria gain\"\"\"\n",
    "    left_child = [i for i in root if x[i, feature_index] == 0]\n",
    "    right_child = [i for i in root if x[i, feature_index] == 1]\n",
    "    \n",
    "    return criteria_func(y, root, left_child, right_child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use information gain criteria to decide how to split the root node of our decision tree.\n",
    "\n",
    "2.2 Complete the `entropy` function. (*5 points*)\n",
    "\n",
    "2.3 Complete the `information_gain_criteria` to compute the information gained by splitting the root node.\n",
    " Print the gain value for splitting based on *Colorado residency* (*5 points*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(y, indices):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param indices: the indices of data points\n",
    "    :return: Returns the entropy in the labels for the data points in indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    entropy_value = 0\n",
    "    if len(indices) == 0: # deal with corner case when there is no data point.\n",
    "        return entropy_value\n",
    "    else:\n",
    "        #BEGIN Workspace 2.2\n",
    "        #TODO: Compute the entropy of the labels from indices\n",
    "        \n",
    "        error_count = get_error_in_leaf(y, indices)\n",
    "        non_error_count = len(indices) - error_count\n",
    "        \n",
    "        p_1 = (error_count/len(indices))\n",
    "        p_2 = (non_error_count/len(indices))\n",
    "        \n",
    "        if p_1 == 0.0 or p_2 == 0.0:\n",
    "            entropy == 0\n",
    "        else:\n",
    "            entropy_value = (p_1*(np.log2(1/p_1))) + (p_2*(np.log2(1/p_2)))\n",
    "        \n",
    "        #END Workspace 2.2\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain_criteria(y, root, left_child, right_child):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param root: indices of all the data points in the root\n",
    "    :param left_child: the subset of indices in the left child\n",
    "    :param right_child: the subset of indices in the right child\n",
    "    :return: information gain of the split\n",
    "    \"\"\"\n",
    "    information_gain = 0\n",
    "    #BEGIN Workspace 2.3.a\n",
    "    #TODO: Compute the information gain of the split\n",
    "    \n",
    "    Entropy_prior = entropy(y, root)\n",
    "    Entropy_left = entropy(y, left_child)\n",
    "    Entropy_right = entropy(y, right_child)\n",
    "    \n",
    "    \n",
    "    left = np.array([y[i] for i in left_child])\n",
    "    right = np.array([y[i] for i in right_child])\n",
    "    \n",
    "    Entropy_split = ((len(left_child)/len(root))*Entropy_left)+((len(right_child)/len(root))*Entropy_right)\n",
    "    \n",
    "    information_gain = Entropy_prior - Entropy_split\n",
    "    \n",
    "    \n",
    "    #END Workspace 2.3.a\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain of split based on colorado residency:  0.091\n"
     ]
    }
   ],
   "source": [
    "feature_id = 2\n",
    "info_gain = 0\n",
    "#BEGIN Workspace 2.3.b\n",
    "#TODO: report the information gain of the split based on Colorado Residency\n",
    "x = features\n",
    "y = labels\n",
    "root = [i for i in range(len(y))]\n",
    "\n",
    "info_gain = value_split_binary_feature(x, y, feature_id, root, criteria_func=information_gain_criteria)\n",
    "\n",
    "print(\"Information gain of split based on colorado residency: \", \"{:.3f}\".format(info_gain))\n",
    "#END Workspace 2.3.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to deal with continuous features for the decision tree.\n",
    "One way to deal with continuous (or ordinal) data is to define binary features based on thresholding as we've done\n",
    "for NaiveBayes. But we have to find the optimal threshold based on the criteria we're using.\n",
    "\n",
    "2.4 Complete the `value_split_continuous_feature` by trying different possible threshold values of feature\n",
    "of index `feature_index` and return the best criteria value and threshold. (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def value_split_continuous_feature(x, y, feature_index, root, criteria_func=information_gain_criteria):\n",
    "    \"\"\"\n",
    "    :param x: all feature values\n",
    "    :param y: all labels\n",
    "    :param feature_index: feature id to split the tree based on\n",
    "    :param root: indexes of all the data points in the root\n",
    "    :param criteria_func: the splitting criteria function\n",
    "    :return: Return the best value and its corresponding threshold by splitting based on a continuous feature.\n",
    "    \"\"\"\n",
    "\n",
    "    best_value, best_thres = 0, 0\n",
    "    \n",
    "    #BEGIN Workspace 2.4\n",
    "    #TODO: Complete the function as detailed in the question and function description\n",
    "    \n",
    "    values = []\n",
    "    thresholds = []\n",
    "    \n",
    "    if feature_index == 0:\n",
    "        \n",
    "        salary_threshold = 53600\n",
    "        \n",
    "        for i in range(1,12):\n",
    "            inc = i*3\n",
    "            age_threshold = 26 + inc\n",
    "            bin_x = threshold_features(x, age_threshold, salary_threshold)\n",
    "            \n",
    "            values.append(value_split_binary_feature(bin_x, y, feature_index, root, criteria_func))\n",
    "            thresholds.append(age_threshold)\n",
    "            \n",
    "    if feature_index == 1:\n",
    "        \n",
    "        age_threshold = 38\n",
    "        \n",
    "        for i in range(1,30):\n",
    "            inc = 3000*i\n",
    "            salary_threshold = 26000 + inc\n",
    "            bin_x = threshold_features(x, age_threshold, salary_threshold)\n",
    "            \n",
    "            values.append(value_split_binary_feature(bin_x, y, feature_index, root, criteria_func))\n",
    "            thresholds.append(salary_threshold)\n",
    "    \n",
    "    sorted_vals = np.sort(values)\n",
    "    best_value = sorted_vals[len(values)-1]\n",
    "    best_value_idx = values.index(best_value)\n",
    "    best_thres = thresholds[best_value_idx]\n",
    "    #END Workspace 2.4\n",
    "    return best_value, best_thres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.5 Find the best thresholds for age and salary. Print their corresponding information gains. (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best age threshold:  59\n",
      "Split information gain (best age threshold):  0.14448434380562825\n",
      "\n",
      "\n",
      "Best salary threshold:  29000\n",
      "Split information gain (best salary threshold):  0.3219280948873624\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = features\n",
    "y = labels\n",
    "root = list(range(len(labels))) # root includes all data points\n",
    "#BEGIN Workspace 2.5\n",
    "#TODO: Report the best thresholds for age and salary and their split information gains\n",
    "\n",
    "age_val_split = value_split_continuous_feature(x, y, 0, root, criteria_func=information_gain_criteria)\n",
    "salary_val_split = value_split_continuous_feature(x, y, 1, root, criteria_func=information_gain_criteria)\n",
    "\n",
    "print(\"Best age threshold: \", age_val_split[1])\n",
    "print(\"Split information gain (best age threshold): \", age_val_split[0])\n",
    "print(\"\\n\")\n",
    "print(\"Best salary threshold: \", salary_val_split[1])\n",
    "print(\"Split information gain (best salary threshold): \", salary_val_split[0])\n",
    "print(\"\\n\")\n",
    "#END Workspace 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Based on the obtained information gains, if we build a decision stump (decision tree with depth 1) greedily,\n",
    "which feature should we choose? Why? What's the resulting accuracy? (*2 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#BEGIN Workspace 2.6.a\n",
    "\n",
    "- Which feature should we pick for the decision stump? Why?\n",
    "\n",
    "    Taking a greedy approach means that we should take the optimal local choice at each node-split. Dealing with building a decision stump is can be thought of as dealing with a single decision split on the root node. Given our greedy approach, we should chose the split based on the feature which maximizes our information gain for such split. \n",
    "\n",
    " The information gains for each feature-split are the following:\n",
    "\n",
    "    a) Age (-with threshold value of 59): 0.144 \n",
    "    b) Salary (-with threshold value of 29000): 0.321\n",
    "    c) Colorado residency:  0.091\n",
    "    \n",
    " Concerning only the initial root-split, choosing the Salary feature (-threshold of 29000) is the optimal local choice.\n",
    " \n",
    "#END Workspace 2.6.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Parent: [1 0 0 1 1 1 1 0 0 1]\n",
      "Left child: [0 0] ,  Right child: [1 0 1 1 1 1 0 1]\n",
      "\n",
      "\n",
      "Left child errors:  0\n",
      "Right child errors:  2\n",
      "\n",
      "\n",
      "Accuracy:  0.8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#BEGIN Workspace 2.6.b\n",
    "#TODO: Split based on the chosen feature and compute the accuracy (use get_error_in_leaf)\n",
    "\n",
    "def split_and_accuracy(x, y, age_threshold, salary_threshold, root):\n",
    "\n",
    "    bin_x = threshold_features(x, age_threshold, salary_threshold)\n",
    "    \n",
    "    left_child = [i for i in root if bin_x[i, 1] == 0]\n",
    "    right_child = [i for i in root if bin_x[i, 1] == 1]\n",
    "    \n",
    "    parent = np.array([y[i] for i in root])\n",
    "    left = np.array([y[i] for i in left_child])\n",
    "    right = np.array([y[i] for i in right_child])\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Parent:\", parent)\n",
    "    print(\"Left child:\", left, \",  Right child:\", right)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    left_errors = get_error_in_leaf(y, left_child)\n",
    "    right_errors = get_error_in_leaf(y, right_child)\n",
    "    \n",
    "    print(\"Left child errors: \", left_errors)\n",
    "    print(\"Right child errors: \", right_errors)\n",
    "    \n",
    "    hits = (len(left_child)-left_errors) + (len(right_child)-right_errors)\n",
    "    accuracy = hits/len(root)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"\\n\")\n",
    "\n",
    "age_threshold = 59\n",
    "salary_threshold = 29000\n",
    "\n",
    "split_and_accuracy(x, y, age_threshold, salary_threshold, root)\n",
    "#BEGIN Workspace 2.6.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question**\n",
    "\n",
    "2.7 You now have all the ingredients to build a decision tree recursively.\n",
    "You can build a decision tree of depth two and report its classification error on the training data and the tree.(*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root [1, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "Left leaf [0, 0] ,  Right child [1, 0, 1, 1, 1, 1, 0, 1]\n",
      "\n",
      "\n",
      "Root [1, 0, 1, 1, 1, 1, 0, 1]\n",
      "Left leaf [1, 1, 1, 1, 1, 0, 1] ,  Right leaf [0]\n",
      "\n",
      "\n",
      "Errors in leaves: 1\n",
      "Accuracy:  0.9\n"
     ]
    }
   ],
   "source": [
    "#BEGIN Workspace 2.7\n",
    "#TODO: Build a Decision Tree of Depth 2 using age, salary and the previously computed thresholds\n",
    "\n",
    "def buildTree(x, y, depth, root, errors):\n",
    "    \n",
    "    left_child = [i for i in root if x[i][depth-1] == 0]\n",
    "    right_child = [i for i in root if x[i][depth-1] == 1]\n",
    "\n",
    "    left = [y[i] for i in left_child]\n",
    "    right = [y[i] for i in right_child]\n",
    "\n",
    "    left_errors = get_error_in_leaf(y, left_child)\n",
    "    right_errors = get_error_in_leaf(y, right_child)\n",
    "   \n",
    "    if depth != 0:\n",
    "        \n",
    "        depth -= 1\n",
    "        \n",
    "        parent = [y[i] for i in root]\n",
    "        print(\"Root\", parent)\n",
    "        \n",
    "        if depth != 0:\n",
    "            \n",
    "            if left_errors == 0 and right_errors != 0:\n",
    "                print(\"Left leaf\",left,\",  Right child\", right)\n",
    "                \n",
    "            if left_errors != 0 and right_errors == 0:\n",
    "                print(\"Left child\",left,\",  Right leaf\", right)\n",
    "        else:\n",
    "            print(\"Left leaf\",left,\",  Right leaf\", right)\n",
    "\n",
    "        if left_errors != 0:\n",
    "            print(\"\\n\")\n",
    "            return buildTree(x,y,depth,left_child, left_errors)\n",
    "\n",
    "        if right_errors != 0:\n",
    "            print(\"\\n\")\n",
    "            return buildTree(x,y,depth,right_child, right_errors)\n",
    "            \n",
    "    else:\n",
    "        return errors\n",
    "        \n",
    "        \n",
    "    \n",
    "bin_x = threshold_features(x, age_threshold, salary_threshold)\n",
    "new_bin_x = []\n",
    "\n",
    "\n",
    "for row in bin_x:\n",
    "    row = list(row)\n",
    "    del row[2]\n",
    "    new_bin_x.append(row)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "errors = 0\n",
    "errors = buildTree(new_bin_x, y,2, root, errors)\n",
    "print(\"Errors in leaves:\", errors)\n",
    "\n",
    "accuracy = (len(y) - errors)/len(y)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "#END Workspace 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3  - Decision Tree Ensembles: Bagging and (BONUS: Boosting) [50 points]\n",
    "---\n",
    "\n",
    "We are going to predict house price levels using decision tree ensembles.\n",
    "\n",
    "In this classification problem, we compare Decision trees and it's ensembles - Bagging and Boosting on House Price prediction [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "Our *weak learner* for this problem is the DecisionTreeClassifier from scikit-learn with `max_depth=10`.\n",
    "\n",
    "We start first by loading preprocessed data that we'll use. Since the original data is for regression, we have first to transform\n",
    "`y_train` and `y_test` to discrete values reflecting price level.\n",
    "\n",
    "|Price range| Label|\n",
    "|:----------:|--:|\n",
    "| $ P < $125000|0|\n",
    "|125000$\\leq P < $ 160000| 1 |\n",
    "|160000$ \\leq P < $ 200000| 2 |\n",
    "|200000$ \\leq P $ | 3 |\n",
    "\n",
    "3.1 Start by transforming`y_train` and `y_test` to discrete values using the provided ranges. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] (1166, 79)\n",
      "[0 1 2 3] (292, 79)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/test_train.pkl','rb'))\n",
    "#BEGIN Workspace 3.1\n",
    "#TODO: Discretize y_train and y_test\n",
    "\n",
    "#print(y_train)\n",
    "#print(y_test)\n",
    "\n",
    "def discretize(data):\n",
    "    \n",
    "    discrete_data = []\n",
    "    \n",
    "    for sample in data:\n",
    "        \n",
    "        if sample < 125000:\n",
    "            discrete_data.append(0)\n",
    "            \n",
    "        if sample >= 125000 and sample < 160000:\n",
    "            discrete_data.append(1)\n",
    "            \n",
    "        if sample >= 160000 and sample < 200000:\n",
    "            discrete_data.append(2)\n",
    "            \n",
    "        if sample >=  200000:\n",
    "            discrete_data.append(3)\n",
    "    \n",
    "    return discrete_data\n",
    "\n",
    "y_train = discretize(y_train)\n",
    "y_test = discretize(y_test)\n",
    "\n",
    "#END Workspace 3.1\n",
    "print(np.unique(y_train), X_train.shape)\n",
    "print(np.unique(y_test), X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.2 Complete the `ensemble_test` class to `fit` the model received as parameter and store the metrics and running time. (*5 points*)\n",
    "\n",
    "3.3 Complete `plot_metric` to show and compare different statistics of each model in a bar chart. (*5 points*)\n",
    "\n",
    "Later we will also use `ensemble_test` class to plot score, metric and time taken to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_weak_learner():\n",
    "    \"\"\"Return a new instance of out chosen weak learner\"\"\"\n",
    "    return DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "class EnsembleTest:\n",
    "    \"\"\"\n",
    "        Test multiple model performance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_train, y_train, x_test, y_test):\n",
    "        \"\"\"\n",
    "        initialize data partitions\n",
    "        \"\"\"\n",
    "        self.scores = {}\n",
    "        self.execution_time = {}\n",
    "        self.metric = {}\n",
    "        self.x_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.score_name ='Mean accuracy'\n",
    "        self.metric_name = 'Precision(micro)'\n",
    "\n",
    "    def fit_model(self, model, name):\n",
    "        \"\"\"\n",
    "        Fit the model on train data.\n",
    "        predict on test and store score and execution time for each fit.\n",
    "        :param model: model\n",
    "        :param name: name of model\n",
    "        \"\"\"\n",
    "        start = time()\n",
    "        #BEGIN Workspace 3.2\n",
    "        #TODO: Fit the model and get the predictions\n",
    "        #       train and test data are treated as global variables\n",
    "        #Hint: self.scores[name] = precision_score(?, average=\"micro\") # in multi-class, micro implies treating it as binary precision\n",
    "        #Hint self.metric[name] = Accuracy\n",
    "        \n",
    "        \n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        \n",
    "        y_pred = model.predict(self.x_test)\n",
    "        \n",
    "        hit, miss = 0, 0\n",
    "        #print(len(y_pred),len(self.y_test))\n",
    "\n",
    "        for (pred_label, label) in zip(y_pred, self.y_test):\n",
    "\n",
    "            if pred_label == label:\n",
    "                hit += 1\n",
    "            else:\n",
    "                miss += 1\n",
    "\n",
    "        Accuracy = hit/(hit+miss)  \n",
    "\n",
    "        self.scores[name] = precision_score(self.y_test, y_pred, average=\"micro\")\n",
    "        self.metric[name] = Accuracy\n",
    "        \n",
    "        \n",
    "        #END Workspace 3.2\n",
    "        self.execution_time[name] = time() - start\n",
    "\n",
    "    def print_result(self):\n",
    "        \"\"\"\n",
    "            print results for all models trained and tested.\n",
    "        \"\"\"\n",
    "        models_cross = pd.DataFrame({\n",
    "            'Model'         : list(self.metric.keys()),\n",
    "             self.score_name     : list(self.scores.values()),\n",
    "             self.metric_name    : list(self.metric.values()),\n",
    "            'Execution time': list(self.execution_time.values())})\n",
    "        print(models_cross.sort_values(by=self.score_name, ascending=False))\n",
    "\n",
    "    def plot_metric(self):\n",
    "        #BEGIN Workspace 3.3\n",
    "        #TODO: plot bar chart for each metric : time, metric, score\n",
    "        times = []\n",
    "        metrics = []\n",
    "        scores =[]\n",
    "        \n",
    "        for time in self.execution_time.values():\n",
    "            times.append(time)\n",
    "            \n",
    "        for metric in self.metric.values():\n",
    "            metrics.append(metric)\n",
    "        \n",
    "        for score in self.scores.values():\n",
    "            scores.append(score)\n",
    "        \n",
    "        plt.bar(range(len(times)),times, color='blue')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.bar(range(len(metrics)),metrics, color='purple')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.bar(range(len(scores)),scores, color='green')\n",
    "        plt.show()\n",
    "            \n",
    "        #END Workspace 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.4 Test `EnsembleTest` using our weak learner returned by `get_weak_learner` (*2 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Mean accuracy  Precision(micro)  Execution time\n",
      "0  DecisionTreeTest       0.681507          0.681507        0.021736\n"
     ]
    }
   ],
   "source": [
    "# create a handler for ensemble_test, use the created handler for fitting different models.\n",
    "ensemble_handler = EnsembleTest(X_train,y_train,X_test,y_test)\n",
    "#BEGIN Workspace 3.4\n",
    "#TODO: Initialize weak learner and fit ensemble_handler\n",
    "model = get_weak_learner()\n",
    "\n",
    "ensemble_handler.fit_model(model, \"DecisionTreeTest\")\n",
    "\n",
    "#END Workspace 3.4\n",
    "ensemble_handler.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging:**\n",
    "\n",
    "Bagging consists of training a set of weak learners using random subsets of the train data.\n",
    "\n",
    "3.5 First, complete `sample_data` to return a random sample of size `sample_ratio * len(X_train)` of features and labels (*2 points*)\n",
    "\n",
    "3.6 Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on random sample of the data (*5 points*)\n",
    "\n",
    "3.7 Complete `predict` method to return the most likely label by combining different estimators predictions. \n",
    "Use a simple majority / plurality vote system similar to the one used in your KNNClassifier in Problem Set 1. However, in this case, to break a tie you should use `predict_log_proba` or `predict_proba` method of DecisionTreeClassifier:\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba) (*2 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaggingEnsemble(object):\n",
    "\n",
    "    def __init__(self, n_estimators, sample_ratio):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.estimators = []\n",
    "        \n",
    "\n",
    "    def sample_data(self, x_train, y_train):\n",
    "        x_sample, y_sample = None, None\n",
    "        #BEGIN Workspace 3.5\n",
    "        #TODO: sample random subset of size sample_ratio * len(X_train)\n",
    "        \n",
    "        size = int(np.floor(self.sample_ratio*len(x_train)))\n",
    "        \n",
    "        sample_idx = np.random.choice(len(x_train), size, replace=True)\n",
    "        \n",
    "        x_sample = [x_train[i] for i in sample_idx]\n",
    "        y_sample = [y_train[i] for i in sample_idx]\n",
    "        \n",
    "        \n",
    "        #END Workspace 3.5\n",
    "        return x_sample, y_sample\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            #BEGIN Workspace 3.6\n",
    "            #TODO: sample data and create new weak learned trained on the sample\n",
    "            \n",
    "            x_sample, y_sample  = self.sample_data(x_train, y_train)\n",
    "            \n",
    "            weak_learner = get_weak_learner()\n",
    "            weak_learner.fit(x_sample, y_sample)\n",
    "            \n",
    "            self.estimators.append(weak_learner)\n",
    "            \n",
    "            \n",
    "            #END Workspace 3.6\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predicted_proba = 0\n",
    "        answer = 0\n",
    "        #BEGIN Workspace 3.7\n",
    "        #TODO: go through the trained estimators and acummualte their predicted_proba to get the mostly likely label\n",
    "        \n",
    "        estimators_labels = []\n",
    "        \n",
    "        for estimator in self.estimators:\n",
    "            estimators_labels.append(estimator.predict_proba(X_test))\n",
    "            \n",
    "        \n",
    "        acc_probs = estimators_labels[0]\n",
    "    \n",
    "        for i in range(1, self.n_estimators):\n",
    "            acc_probs += estimators_labels[i]\n",
    "        \n",
    "        likely_labels = []\n",
    "        \n",
    "        for row in acc_probs:\n",
    "            max_p = max(row)\n",
    "            label = np.where(row==max_p)\n",
    "            likely_labels.append(label[0].item(0))\n",
    "            \n",
    "        answer = likely_labels    \n",
    "        #END Workspace 3.7\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Mean accuracy  Precision(micro)  Execution time\n",
      "1           Bagging       0.739726          0.739726        0.158844\n",
      "0  DecisionTreeTest       0.681507          0.681507        0.021736\n"
     ]
    }
   ],
   "source": [
    "# This cell should run without errors\n",
    "ensemble_handler.fit_model(BaggingEnsemble(10, 0.9),'Bagging')\n",
    "ensemble_handler.print_result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Random Forest**\n",
    "Random Forest has an additional layer of randomness compared to Bagging: we also sample a random subset of the features.\n",
    "\n",
    "3.8 First, complete `sample_data` to return a random sample of size `sample_ratio * len(X_train)` of labels and `feature_ratio * num_features` of features (*2 points*)\n",
    "\n",
    "3.9 Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on random sample of the data.\n",
    "Make sure to keep track of the sampled features for each estimator to use them in the prediction step. (*5 points*)\n",
    "\n",
    "3.10 Complete `predict` method to return the most likely label by combining different estimators predictions.\n",
    "Use a simple majority / plurality vote system similar to the one used in your KNNClassifier in Problem Set 1. However, in this case, to break a tie you should use `predict_log_proba` or `predict_proba`  method of DecisionTreeClassifier:\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba) (*3 points*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "\n",
    "    def __init__(self, n_estimators, sample_ratio, features_ratio):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.features_ratio = features_ratio\n",
    "        self.estimators = []\n",
    "        self.features_indices = []\n",
    "\n",
    "    def sample_data(self, x_train, y_train):\n",
    "        X_sample, y_sample, features_indices = None, None, None\n",
    "        #BEGIN Workspace 3.8\n",
    "        #TODO: sample random subset of size sample_ratio * len(X_train) and subset of features of size\n",
    "        #         features_ratio * num_features\n",
    "        \n",
    "        samples_size = int(np.floor(self.sample_ratio*len(x_train)))\n",
    "        features_size = int(np.floor(self.features_ratio*len(x_train[0])))\n",
    "        \n",
    "        samples_idx = np.random.choice(len(x_train), samples_size)\n",
    "        features_indices = np.random.choice(len(x_train[0]), features_size)\n",
    "        \n",
    "        x_sample = [x_train[i] for i in samples_idx]\n",
    "        y_sample = [y_train[i] for i in samples_idx]\n",
    "        \n",
    "\n",
    "        #END Workspace 3.8\n",
    "        return x_sample, y_sample, features_indices\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        for _ in range(self.n_estimators):\n",
    "            #BEGIN Workspace 3.9\n",
    "            #TODO: sample data with random subset of rows and features using sample_data\n",
    "            #Hint: keep track of the features indices in features_indices to use in predict\n",
    "            \n",
    "            x_samples, y_samples, features_idx  = self.sample_data(x_train, y_train)\n",
    "            \n",
    "            self.features_indices.append(features_idx)\n",
    "            \n",
    "            x_samples_rforest = []\n",
    "            \n",
    "            for sample in x_samples:\n",
    "                \n",
    "                x_smpl = [sample[i] for i in features_idx]\n",
    "                x_samples_rforest.append(x_smpl)\n",
    "            \n",
    "            weak_learner = get_weak_learner()\n",
    "            weak_learner.fit(x_samples_rforest, y_samples)\n",
    "            \n",
    "            self.estimators.append(weak_learner)\n",
    "            \n",
    "            #END Workspace 3.9\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predicted_proba = 0\n",
    "        answer = 0\n",
    "        #BEGIN Workspace 3.10\n",
    "        #TODO: compute cumulative sum of predict proba from estimators and return the labels with highest likelihood\n",
    "        \n",
    "        \n",
    "        estimators_labels = []\n",
    "        inc = 0\n",
    "        \n",
    "        for estimator in self.estimators:\n",
    "            x_test_rforest = X_test[:,self.features_indices[inc]]\n",
    "            estimators_labels.append(estimator.predict_proba(x_test_rforest))\n",
    "            inc+=1\n",
    "        \n",
    "        acc_probs = estimators_labels[0]\n",
    "    \n",
    "        for i in range(1, self.n_estimators):\n",
    "            acc_probs += estimators_labels[i]\n",
    "        \n",
    "        likely_labels = []\n",
    "        \n",
    "        for row in acc_probs:\n",
    "            max_p = max(row)\n",
    "            label = np.where(row==max_p)\n",
    "            likely_labels.append(label[0].item(0))\n",
    "            \n",
    "        answer = likely_labels \n",
    "        \n",
    "        #END Workspace 3.10\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Mean accuracy  Precision(micro)  Execution time\n",
      "2      RandomForest       0.770548          0.770548        0.695282\n",
      "1           Bagging       0.739726          0.739726        0.158844\n",
      "0  DecisionTreeTest       0.681507          0.681507        0.021736\n"
     ]
    }
   ],
   "source": [
    "# This cell should run without errors\n",
    "ensemble_handler.fit_model(RandomForest(20, sample_ratio=0.9, features_ratio=0.8), 'RandomForest')\n",
    "ensemble_handler.print_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
