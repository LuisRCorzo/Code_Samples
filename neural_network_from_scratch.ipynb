{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec27d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48d2880",
   "metadata": {},
   "source": [
    "# Model Definition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f2d2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        \"\"\"\n",
    "        Neural Network\n",
    "        \n",
    "        sizes: list [input_dimensions, hidden_layer_dimensions, output_dimensions]\n",
    "        L: length of the layer\n",
    "        biases: list containing biases values for each layer\n",
    "        weights: list containing weights for each layer\n",
    "        \n",
    "        Parameters:\n",
    "        sizes: list containing dimenions of the neual network\n",
    "        \"\"\"\n",
    "        self.L = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(n, 1) for n in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(n, m) for (m, n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        self.acc_train_array = []\n",
    "        self.acc_test_array = []\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, z, threshold=20):\n",
    "        \n",
    "        \"\"\"\n",
    "        Sigmoid activation function\n",
    "        \"\"\"\n",
    "        y = 1 / (1 + np.exp(-(z)))\n",
    "\n",
    "        return y\n",
    " \n",
    "\n",
    "    def g_prime(self, z):\n",
    "        \n",
    "        \"\"\"\n",
    "        Derivative of sigmoid activation function\n",
    "        \"\"\"\n",
    "        sig = self.sigmoid(z)\n",
    "        \n",
    "        y = sig*(1-sig)\n",
    "        \n",
    "        return y\n",
    " \n",
    "        \n",
    "    def forward_prop(self, a):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward propagation: \n",
    "        : Do layerwise dot product between the input and the weights, \n",
    "        : adding the coresponding biases and take activations of it \n",
    "        : starting from the first layer then forward and return the final output.\n",
    "        \"\"\"\n",
    "        X = a\n",
    "        \n",
    "        activations = []\n",
    "        \n",
    "        for layer_w, layer_b  in zip(self.weights, self.biases):\n",
    "            \n",
    "            z = np.dot(layer_w, X) + layer_b\n",
    "            a = self.sigmoid(z)\n",
    "            X = a\n",
    "                \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def cost(self, yhat, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        Cost Function\n",
    "        : Cost(a,y) = (yhat-y)^2/2\n",
    "        \"\"\"\n",
    "        return 0.5*np.square(yhat-y)\n",
    "    \n",
    "    \n",
    "    def grad_cost(self, yhat, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        Gradient of cost function:\n",
    "        : Derivative of Cost(yhat,y) \n",
    "        \"\"\"\n",
    "        return (yhat - y)\n",
    "    \n",
    "    \n",
    "    def log_train_progress(self, train, test, epoch):\n",
    "        \n",
    "        \"\"\" Logs training progress. \n",
    "        \"\"\"\n",
    "        acc_train = self.evaluate(train)\n",
    "        self.acc_train_array.append(acc_train)\n",
    "        \n",
    "        if test is not None:\n",
    "            \n",
    "            acc_test = self.evaluate(test)\n",
    "            self.acc_test_array.append(acc_test)\n",
    "            \n",
    "            print(\"Epoch {:4d}: Train acc {:10.5f}, Test acc {:10.5f}\".format(epoch+1, acc_train, acc_test))\n",
    "            \n",
    "        else:\n",
    "            print(\"Epoch {:4d}: Train acc {:10.5f}\".format(epoch+1, acc_train))\n",
    "                 \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        Back propagation for computing the gradients\n",
    "        \n",
    "        NOTE: The back_prop implements it's own forward_prop\n",
    "        \n",
    "        : Once forward prop completes (implemented inside), initate list of gradients (dws, dbs),\n",
    "        : where each element of list stores the corresponding gradients of that layer.\n",
    "        : For each layer compute the gradients and update the list (dws, dbs) and return it.\n",
    "        \n",
    "        Parameters:\n",
    "            x: Sample features\n",
    "            y: Sample labels\n",
    "            \n",
    "        RETURN: (dws, dbs)\n",
    "                 \n",
    "                 dws: list of layerwise derivative of weights\n",
    "                 dbs: list of layerwise derivative of biases\n",
    "        \"\"\"      \n",
    "        \n",
    "        a = x\n",
    "        \n",
    "        # List initialized for storing layer-wise output before it is fed to activations        \n",
    "        pre_activations = [np.zeros(a.shape)]          \n",
    "        \n",
    "        # List initialized for storing layer-wise activations\n",
    "        activations = [a]\n",
    "        \n",
    "        # Forward propogation to compute layer-wise pre_activations and activations\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            \n",
    "            z = np.dot(W,a) + b\n",
    "            pre_activations.append(z)\n",
    "            a = self.sigmoid(z)\n",
    "            activations.append(a)\n",
    "            \n",
    "            #print('W: ',W,\"\\n\")\n",
    "            #print('b: ',b,'\\n')\n",
    "            #print('z: ',z,'\\n')\n",
    "            #print('a: ',a,'\\n')\n",
    "            \n",
    "        \n",
    "        new_activations = []\n",
    "        new_pre_activations = []\n",
    "        \n",
    "        for activations_l, pre_activations_l in zip(activations,pre_activations):\n",
    "            \n",
    "            new_activations.append(activations_l.ravel())\n",
    "            new_pre_activations.append(pre_activations_l.ravel())\n",
    "            \n",
    "        #print(\"layer activations: \", new_activations,'\\n')\n",
    "        #print(\"layer pre_activations: \",  new_pre_activations,'\\n')\n",
    "        \n",
    "#_______ compute Layer 2 gradients: __________________________________\n",
    "        \n",
    "        dws =[]\n",
    "        dbs = []    \n",
    "        \n",
    "        dCost_dY_l2 = []\n",
    "        dOut_dIn_l2 = []\n",
    "        dIn_dw_l2 = []\n",
    "        \n",
    "        y_preds = new_activations[-1]\n",
    "        \n",
    "        for y_pred, y_true in zip(y_preds,y):\n",
    "            \n",
    "            dCost_dY_l2.append(self.grad_cost(y_pred,y_true[0]))\n",
    "            dout_din = self.g_prime(y_pred)\n",
    "            dOut_dIn_l2.append(dout_din)\n",
    "            \n",
    "            \n",
    "        prev_outputs = new_activations[-2]\n",
    "\n",
    "        for a_prev in prev_outputs:\n",
    "            dIn_dw_l2.append(a_prev)\n",
    "        \n",
    "        \n",
    "        #print('dC/dYL: ',dCost_dY_l2,'\\n', 'da/dz: ',dOut_dIn_l2, '\\n', 'dz/dw: ',dIn_dw_l2,'\\n')\n",
    "        \n",
    "        dCost_dwL = []\n",
    "        \n",
    "        dCost_dwL2 = []\n",
    "        \n",
    "        for dC_dy, da_dz in zip(dCost_dY_l2, dOut_dIn_l2):\n",
    "            \n",
    "            dCost_dw_l2 = []\n",
    "            \n",
    "            for dz_dw in dIn_dw_l2:\n",
    "                #print('dCost/dyi: ',dC_dy,'\\n')\n",
    "                dCost_dw_l2.append(dC_dy*da_dz*dz_dw) \n",
    "                \n",
    "            dCost_dwL2.append(dCost_dw_l2) \n",
    "            \n",
    "        dCost_dwL2 = np.asarray(dCost_dwL2)\n",
    "        #print(type(dCost_dwL2))\n",
    "        \n",
    "#_______ compute layer 1 gradients: _________________________________\n",
    "\n",
    "        dCost_dH_l1 = []\n",
    "        dOut_dIn_l1 = []\n",
    "        dIn_dw_l1 = []\n",
    "        \n",
    "        local_grads_l2 = []\n",
    "        \n",
    "        for i in range(0, self.sizes[2]):\n",
    "            \n",
    "            local_grads_l2.append(dCost_dY_l2[i]*dOut_dIn_l2[i])\n",
    "            \n",
    "        \n",
    "        w_l2 = self.weights[-1]\n",
    "        \n",
    "        for i in range(0, self.sizes[1]):\n",
    "            \n",
    "            dCost_dhi = 0\n",
    "            \n",
    "            for loc_grad_i, w_yi in zip(local_grads_l2, w_l2):\n",
    "                \n",
    "                dCost_dhi+=(loc_grad_i*w_yi[i])\n",
    "                \n",
    "            dCost_dH_l1.append(dCost_dhi)\n",
    "            dOut_dIn_l1.append(prev_outputs[i]*(1-prev_outputs[i]))\n",
    "        \n",
    "        \n",
    "        for out_l0 in new_activations[0]:\n",
    "            dIn_dw_l1.append(out_l0)\n",
    "            \n",
    "        \n",
    "        #print('\\n')\n",
    "        #print('dC_dh: ',dCost_dH_l1)\n",
    "        #print('dh_dz: ',dOut_dIn_l1)\n",
    "        #print('dz_dw: ', dIn_dw_l1)\n",
    "        #print('\\n')\n",
    "\n",
    "        dCost_dwL1 =[]\n",
    "        loc_grads_l1 = []\n",
    "        \n",
    "        for dC_dh, da_dz in zip(dCost_dH_l1, dOut_dIn_l1):\n",
    "            \n",
    "            loc_grads_l1.append(dC_dh*da_dz)\n",
    "            \n",
    "            dCost_dw_l1 = []\n",
    "            \n",
    "            for dz_dw in dIn_dw_l1:\n",
    "                \n",
    "                dCost_dw_l1.append(dC_dh*da_dz*dz_dw)\n",
    "                \n",
    "            dCost_dwL1.append(dCost_dw_l1)\n",
    "        \n",
    "        #print(np.asarray(dCost_dwL1))\n",
    "        \n",
    "        dCost_dwL.append(np.asarray(dCost_dwL1))\n",
    "        dCost_dwL.append(dCost_dwL2)\n",
    "        \n",
    "        dbs.append(np.asarray(loc_grads_l1))\n",
    "        dbs.append(np.asarray(local_grads_l2))\n",
    "        \n",
    "        #print(dCost_dwL)\n",
    "        #print(dbs)\n",
    "        return dCost_dwL, dbs\n",
    "        \n",
    "\n",
    "\n",
    "    def SGD_step(self, x, y, eta):\n",
    "        \"\"\"\n",
    "        Update the values of weights (self.weights) & biases (self.biases)\n",
    "        : Get values of gradients (dws, dbs) by calling back_prop \n",
    "        : and update parameters using obtained gradients & learning rate eta\n",
    "        \n",
    "        Parameters:\n",
    "            x: single sample features.\n",
    "            y: single sample target.\n",
    "            eta: learning rate.\n",
    "            lam: Regularization parameter.\n",
    "                \n",
    "        RETURN: none\n",
    "        \"\"\"\n",
    "        \n",
    "        dws_dbs = self.back_prop(x,y)\n",
    "        \n",
    "        dws = dws_dbs[0]\n",
    "        dbs = dws_dbs[1]\n",
    "        \n",
    "        dws_l1 = dws[0]\n",
    "        dws_l2 = dws[1]\n",
    "        \n",
    "        dbs_l1 = dbs[0]\n",
    "        dbs_l2 = dbs[1]\n",
    "        \n",
    "        ws_l1 = self.weights[0]\n",
    "        ws_l2 = self.weights[1]\n",
    "        \n",
    "        bs_l1 = self.biases[0]\n",
    "        bs_l2 = self.biases[1]\n",
    "        \n",
    "        \n",
    "        #print('-Before step W:',self.weights,'\\n')\n",
    "        #print('-Before step B:',self.biases,'\\n')\n",
    "        \n",
    "        new_w_l2 = []\n",
    "        \n",
    "        for neuron_ws_l2, neuron_dws_l2 in zip(ws_l2, dws_l2):\n",
    "            \n",
    "            new_neuron_ws_l2 = []\n",
    "            \n",
    "            for w_l2, dw_l2 in zip(neuron_ws_l2, neuron_dws_l2):\n",
    "                \n",
    "                new_weight_l2 = w_l2 - (eta*dw_l2)\n",
    "                new_neuron_ws_l2.append(new_weight_l2)\n",
    "            \n",
    "            new_w_l2.append(new_neuron_ws_l2)\n",
    "        \n",
    "        new_w_l2 = np.asarray(new_w_l2)\n",
    "        #print('-->',new_w_l2)\n",
    "        \n",
    "        \n",
    "        new_w_l1 = []\n",
    "        \n",
    "        for neuron_ws_l1, neuron_dws_l1 in zip(ws_l1, dws_l1):\n",
    "            \n",
    "            new_neuron_ws_l1 = []\n",
    "            \n",
    "            for w_l1, dw_l1 in zip(neuron_ws_l1, neuron_dws_l1):\n",
    "                \n",
    "                new_weight_l1 = w_l1 - (eta*dw_l1)\n",
    "                new_neuron_ws_l1.append(new_weight_l1)\n",
    "            \n",
    "            new_w_l1.append(new_neuron_ws_l1)\n",
    "        \n",
    "        new_w_l1 = np.asarray(new_w_l1)\n",
    "        #print('-->',new_w_l1)\n",
    "        \n",
    "        new_W = np.asarray([new_w_l1, new_w_l2], dtype=object)\n",
    "        #print(new_W)\n",
    "        \n",
    "        \n",
    "        new_b_l2 = []\n",
    "        \n",
    "        #print(bs_l2, dbs_l2)\n",
    "        \n",
    "        for b_l2, db_l2 in zip(bs_l2, dbs_l2):\n",
    "            \n",
    "            new_bias_l2 = b_l2[0] - (eta*db_l2)\n",
    "            new_b_l2.append([new_bias_l2])\n",
    "        \n",
    "        new_b_l2 = np.asarray(new_b_l2)\n",
    "        \n",
    "        \n",
    "        new_b_l1 = []\n",
    "        \n",
    "        for b_l1, db_l1 in zip(bs_l1, dbs_l1):\n",
    "\n",
    "            new_bias_l1 = b_l1[0] - (eta*db_l1)\n",
    "            new_b_l1.append([new_bias_l1])\n",
    "        \n",
    "        new_b_l1 = np.asarray(new_b_l1)\n",
    "        \n",
    "        \n",
    "        new_B = np.array([new_b_l1, new_b_l2], dtype=object)\n",
    "        \n",
    "        \n",
    "        self.weights = new_W\n",
    "        self.biases = new_B\n",
    "        \n",
    "        #print(len(new_W[0][0]))\n",
    "        #print('-After step W:',self.weights,'\\n')\n",
    "        #print('-After step B:',self.biases,'\\n')\n",
    "        \n",
    "\n",
    "    def train(self, train, epochs, eta, verbose=True, test=None):\n",
    "        \"\"\"\n",
    "        Training routine for the neural network\n",
    "        : For each epoch the following is done:\n",
    "        : shuffle the training dataset.\n",
    "        : call self.SGD_step which will in turn call backprop & update parameters\n",
    "        : Call self.log_train_progress according to the verbose\n",
    "        \n",
    "        Parameters:\n",
    "        \n",
    "            train: Training set -> list containing tuple (Training Feature, Training label)\n",
    "            epochs: Number of epocs to run\n",
    "            eta: Learning rate\n",
    "            verbose: True to print accuracy updates, False otherwise\n",
    "            test: Test set -> list containing tuple (Test Feature, Test label)\n",
    "        \"\"\"\n",
    "        n_train = len(train)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            perm = np.random.permutation(n_train)\n",
    "            \n",
    "            for kk in range(n_train):\n",
    "                self.SGD_step(*train[perm[kk]], eta)\n",
    "                \n",
    "            if verbose and epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "                self.log_train_progress(train, test, epoch)\n",
    "    \n",
    "    def evaluate(self, test):\n",
    "        \"\"\"\n",
    "        Evaluate current model \n",
    "        : computes the fraction of labels matching \n",
    "        test : (test_x, test_y)\n",
    "        \"\"\"\n",
    "        ctr = 0\n",
    "        for x, y in test:\n",
    "            yhat = self.forward_prop(x)\n",
    "            ctr += yhat.argmax() == y.argmax()\n",
    "            \n",
    "        return float(ctr) / float(len(test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ee77e",
   "metadata": {},
   "source": [
    "# Load Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a4e28e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Input Features (14x14):  196\n",
      "Number of Output classes:  10 \n",
      "\n",
      "Original Tiny MNIST dataset: \n",
      "Train set len:  2499\n",
      "Test set len:  2499 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset:\n",
    "\n",
    "def TinyMNIST():\n",
    "    location = './data/tinyMNIST.pkl.gz'\n",
    "    f = gzip.open(location, 'rb')\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    train, test = u.load()\n",
    "    return train, test\n",
    "\n",
    "train, test = TinyMNIST()\n",
    "\n",
    "input_dimensions, output_dimensions = len(train[0][0]), len(train[0][1])\n",
    "\n",
    "print('Number of Input Features (14x14): ', input_dimensions)\n",
    "print('Number of Output classes: ', output_dimensions,'\\n')\n",
    "\n",
    "print('Original Tiny MNIST dataset: ')\n",
    "print('Train set len: ', len(train))\n",
    "print('Test set len: ', len(test), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf498f",
   "metadata": {},
   "source": [
    "# Pre-process Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85b66ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after merging train-test, reshuffling, and 60%/20%/20% split: \n",
      "Train set len:  2998\n",
      "Validation set len:  1000\n",
      "Test set len:  1000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pre-process dataset for reshuffling and 60%/20%/20% split:\n",
    "new_data = train+test\n",
    "\n",
    "new_X =[]\n",
    "new_Y =[]\n",
    "\n",
    "for sample in new_data:\n",
    "    \n",
    "    new_X.append(sample[0])\n",
    "    new_Y.append(sample[1])\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_X, new_Y, test_size=0.20, random_state=42)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print('Dataset after merging train-test, reshuffling, and 60%/20%/20% split: ')\n",
    "print('Train set len: ', len(x_train))\n",
    "print('Validation set len: ', len(x_val))\n",
    "print('Test set len: ', len(x_test), '\\n')\n",
    "\n",
    "new_train = [(x,y) for x,y in zip(x_train,y_train)]\n",
    "new_val = [(x,y) for x,y in zip(x_val,y_val)]\n",
    "new_test = [(x,y) for x,y in zip(x_test,y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c69e98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample targets are One-Hot Encoded vectors (0-9):  \n",
      " [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "Image for digit 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#________ Sample example: ___________\n",
    "index=0\n",
    "digit_image = train[index][0]\n",
    "\n",
    "image = digit_image.reshape(14, -1)\n",
    "\n",
    "print(\"Sample targets are One-Hot Encoded vectors (0-9): \",'\\n', train[index][1])\n",
    "\n",
    "print(\"Image for digit {}\".format(list(train[index][1]).index(1)))\n",
    "plt.matshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1: Train acc    0.73149, Test acc    0.71100\n",
      "Epoch   10: Train acc    0.91928, Test acc    0.86900\n",
      "Epoch   20: Train acc    0.95397, Test acc    0.88000\n",
      "Epoch   30: Train acc    0.96965, Test acc    0.89800\n",
      "Epoch   40: Train acc    0.97999, Test acc    0.89900\n"
     ]
    }
   ],
   "source": [
    "model = Network([196,98,10])\n",
    "\n",
    "model.train(new_train,100,0.05,True,new_val)\n",
    "\n",
    "test_accuracy = model.evaluate(new_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0666c56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test-set: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1280d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
